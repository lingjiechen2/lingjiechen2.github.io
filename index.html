<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="images/orange_icon_3.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Lingjie Chen</title>
  <meta name="Lingjie Chen's Homepage" http-equiv="Content-Type" content="Lingjie Chen's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>
 
<body>
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Lingjie Chen 「陈凌杰」</pageheading><br>
  </p>

  <tr>
    <td width="30%" valign="top"><a href="images/LingjieChen.jpg"><img src="images/LingjieChen.jpg" width="100%" style="border-radius:15px"></a>
    <p align=center>
    | <a href="data/LingjieChen_CV.pdf">CV</a> |
    <a href="mailto:lingjiechen127@gmail.com">Email</a> 
    <!-- <a href="https://scholar.google.com/citations?user=wMcPTbEAAAAJ&sortby=pubdate">G Scholar</a> |
    <br/>  -->
    | <a href="https://github.com/lingjiechen2">Github</a> | 
    <!-- <a href="https://twitter.com/zipengfu">Twitter</a> | -->
    <a href="https://www.linkedin.com/in/lingjie-chen-b98a64290/">LinkedIn</a> |
    </p>
    </td>
    <td width="70%" valign="top" align="justify">
    <p>
      Hi there, I am Jason Chen! Currently I am a junior in Data Science at Fudan University.  I am now working in the <a href="https://open-moss.com/en/">Fudan NLP Group</a>, advised by <a href="https://xpqiu.github.io/">Xipeng Qiu</a>
    </p>
    <p>
      My research interests lies in Trustwothy LLM and Mechanistic Interptibility. My goal is to make LLM more acceptable and transparent. To do so, I am devoted to dismysifying the black-box nature of LLM and also giving it constrains so as to prevent being misused.
    </p>
    <p>
      I prefer to seek an PhD opportunity after my graduation.
    </p>
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
   <td width="30%" valign="top" align="center"><a href="https://manipulation-locomotion.github.io">
    <img src="images/pun2024.png" alt="sym" width="100%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></img>
    </a></td>
    <td width="70%" valign="top">
      <p><a href="https://manipulation-locomotion.github.io" id="MANIPLOCO">
      <heading>"A good pun is its own reword": Can Large Language Models Understand Puns?</heading></a><br>
      Zhijun Xu, Siyu Yuan, Lingjie Chen, Deqing Yang<br>
      EMNLP 2024 Under Reivew<br>
      <!-- <b style="color:rgb(255, 100, 100);">Best Systems Paper Award Finalist (top 4)</b> -->
      </p>

      <div class="paper" id="maniploco">
      <!-- <a href="https://manipulation-locomotion.github.io">webpage</a> | -->
      <a href="https://arxiv.org/pdf/2404.13599">pdf</a> |
      <a href="javascript:toggleblock('maniploco_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('maniploco')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2404.13599">arXiv</a> |
      <a href="https://openreview.net/forum?id=8xfJKMzvrm#discussion">OpenReview</a> |
      <!-- <a href="https://www.youtube.com/watch?v=i9EdPl8uJUA">video</a> -->

      <p align="justify"> <i id="maniploco_abs">As one of the common rhetorical devices, puns play a vital role in linguistic study, including the comprehensive analysis of linguistic humor. Although large language models (LLMs) have been widely explored on various tasks of natural language understanding and generation, their ability to understand puns has not been systematically studied, limiting the utilization of LLMs in creative writing and humor creation. In this paper, we leverage three popular tasks, i.e., pun recognition, pun explanation, and pun generation, to systematically evaluate LLMs’ capability of understanding puns. In addition to the evaluation metrics adopted by prior research, we introduce some new evaluation methods and metrics that are better suited to the in-context learning paradigm of LLMs. These new metrics offer a more rigorous assessment of an LLM’s capability to understand puns and align more closely with human cognition. Our research findings reveal the “lazy pun generation” pattern and identify the primary challenges in understanding puns with LLMs.</i></p>
        </div>
      </td>
    </tr>
  
    <tr>
      <td width="40%" valign="top" align="center"><a href="data/IEEE_IoT2019_Energy.pdf"><img src="images/conceptual_model_demo.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></a></td>
      <td width="60%" valign="top">
        <p><a href="data/IEEE_IoT2019_Energy.pdf" id="IEEE_IoT">
        <heading>A Mechanistic View of Transferred and Intrinsic Multilingualism in Large Language Models</heading></a><br>
        Fukang Zhu*, Lingjie Chen*, Ningyu Xu, Xuyang Ge, Junxuan Wang, Zhengfu He, Xipeng Qiu<br>
        EMNLP 2024 Under Reivew</b>
        </p>

        <div class="paper" id="ieee_iot">
        <a href="data/IEEE_IoT2019_Energy.pdf">pdf</a> |
        <a href="javascript:toggleblock('ieee_iot_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('ieee_iot')" class="togglebib">bibtex</a>

        <p align="justify"><i id="ieee_iot_abs">It has been conjectured that multilingual models process information in low-resource languages in an English state of mind since English takes up a large proportion of the training corpus. Recent progress in language model multilingualism provides more evidence for this hypothesis. We ask a further question: \textbf{What if the model is trained on more than one high-resource language?} By studying language models trained mostly on Chinese and English with an interpretability technique called Sparse Autoencoders, we manage to identify a three-stage process of how models think in these two languages. The model first 'detokenize' inputs and both languages are aligned. The representation of these two languages then diverges and processed independently in a 'conceptual stage' and is aligned again in the 'retokenization stage'. We name this after the \textit{Intrinsic Multilingualism}. We empirically test our hypothesis by intervening the model internal with Sparse Autoencoders trained on another language and find that the 'conceptual stage' is crucial for the model to think in different languages. We also showcase a number of features detecting intriguing lingual and cultural bias in Chinese and English.</i></p>

<pre xml:space="preserve">
@article{yao2019energy,
  author = {Yao, Donghuan and Liang, Xiaohui and Fu, Zipeng and Zhang, Kai and Yang, Baojia},
  journal= {IEEE Internet of Things Journal},
  year   = {2019}
}
</pre>
      </div>
    </td>
  </tr>

</table>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Projects</sectionheading></td></tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td width="100%" valign="top">
      <p><a href="https://mobile-aloha.github.io" id="MOBILE_ALOHA">
      <heading>Trade-off: A Lightweight Method to Question Answering Using BERT</heading></a><br>
      Lingjie Chen,
      2024<br>
      </p>

      <div class="paper" id="mobile_aloha">
      <a href="https://github.com/lingjiechen2/STAT_154">webpage</a> |
      <a href="https://github.com/lingjiechen2/STAT_154/blob/main/Trade_off__A_Light_Weight_Model_Solving_QA.pdf">pdf</a> |
      <a href="javascript:toggleblock('QA_BERT_abs')">abstract</a> |
      <a href="https://github.com/lingjiechen2/STAT_154/tree/main/Code">code</a> |
     

      <p align="justify"> <i id="QA_BERT_abs">This research investigates the efficacy of a lightweight, monolingual Bert model specifically tailored for Chinese Question Answering (QA) tasks. By employing a range of advanced training techniques, the model achieved a notable milestone, placing within the top 20 in Kaggle’s renowned QA competition. Our findings reveal a surprisingly narrow performance gap between the specialized Bert model and larger Language Learning Models (LLMs), highlighting the potential of focused, language-specific approaches in the realm of natural language processing.</i></p>
      </div>
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right">
    Website template from <a href="http://www.cs.berkeley.edu/~barron/">here</a> and <a href="http://www.cs.cmu.edu/~dpathak/">here</a>
    </font></p></td></tr>
</table>


  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('material_review_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ieee_iot_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acm_turc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('aog_mcts_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pragmatics_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('collab_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('rma_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('energyloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('navloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('maniploco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('parkour_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('mobile_aloha_abs');
</script>
</body>

</html>
